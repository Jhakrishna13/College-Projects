# -*- coding: utf-8 -*-
"""MAIN Copy of deepfake3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A_BwCoaizCVwZzTDauJb4hiQ_mINk-Bk
"""
#!pip install -U --upgrade tensorflow

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

from google.colab import drive
import os
drive.mount('/content/drive')

from tensorflow import keras
import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import numpy as np
import imageio
import cv2
import os

print("All modules imported successfully!")

DATA_FOLDER = '/content/drive/Shareddrives/DATASET/deepfake-detection-challenge'
TRAIN_SAMPLE_FOLDER = '/content/drive/Shareddrives/DATASET/deepfake-detection-challenge/train_sample_videos'
TEST_FOLDER = '/content/drive/Shareddrives/DATASET/deepfake-detection-challenge/test_videos'

print(f"Train samples: {len(os.listdir(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)))}")
print(f"Test samples: {len(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER)))}")

train_sample_metadata = pd.read_json('/content/drive/Shareddrives/DATASET/deepfake-detection-challenge/train_sample_videos/metadata.json').T
train_sample_metadata.head()

train_sample_metadata.shape

fake_train_sample_video = list(train_sample_metadata[train_sample_metadata.label == 'FAKE'].head(3).index)
print(fake_train_sample_video)

def display_image_from_video(video_path, save_dir):
    '''
    Captures every 10th frame from the video and saves it to the specified directory.
    Displays the first frame and prints number of frames extracted.

    Parameters:
    - video_path: path to input video
    - save_dir: directory where frames will be saved
    '''

    if not os.path.exists(video_path):
        print(f"Error: File {video_path} not found!")
        return

    # Create the save directory if it doesn't exist
    os.makedirs(save_dir, exist_ok=True)

    capture_image = cv2.VideoCapture(video_path)
    frame_count = 0
    extracted_count = 0
    first_frame_saved = None

    while True:
        ret, frame = capture_image.read()
        if not ret or frame is None:
          print(f"Error: No more frames from {video_path}")
          break

        if frame_count % 10 == 0:
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame_filename = os.path.join(save_dir, f"frame_{frame_count}.jpg")
            imageio.imwrite(frame_filename, frame_rgb)

            if first_frame_saved is None:
                first_frame_saved = frame_rgb  # Save first extracted frame for display

            extracted_count += 1

        frame_count += 1

    capture_image.release()

    print(f"Extracted {extracted_count} frames from {video_path}")

    if first_frame_saved is not None:
        plt.figure(figsize=(10, 10))
        plt.imshow(first_frame_saved)
        plt.axis("off")
        plt.title("First Extracted Frame")
        plt.show()

for video_file in fake_train_sample_video:
    video_path = os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file)
    save_frames_dir = os.path.join(DATA_FOLDER, 'extracted_frames', video_file.split('.')[0])  # Unique folder for each video
    print(f"\nProcessing: {video_path}")
    display_image_from_video(video_path, save_frames_dir)

real_train_sample_video = list(train_sample_metadata[train_sample_metadata.label == 'REAL'].head(3).index)
print(real_train_sample_video)

for video_file in real_train_sample_video:
    video_path = os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file)
    save_frames_dir = os.path.join(DATA_FOLDER, 'extracted_frames', video_file.split('.')[0])  # Unique folder per video

    print(f"\nProcessing: {video_path}")
    display_image_from_video(video_path, save_frames_dir)

train_sample_metadata['original'].value_counts()[0:5]

def display_image_from_video_list(video_path_list, video_folder=TRAIN_SAMPLE_FOLDER):
    '''
    input: video_path_list - path for video
    process:
        0. for each video in the video path list
        1. perform a video capture from the video
        2. read the image
        3. display the image
    '''
    plt.figure()
    fig, ax = plt.subplots(2,3,figsize=(16,8))
    # we only show images extracted from the first 6 videos
    for i, video_file in enumerate(video_path_list[0:6]):
        video_path = os.path.join(DATA_FOLDER, video_folder,video_file)
        capture_image = cv2.VideoCapture(video_path)
        ret, frame = capture_image.read()
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        ax[i//3, i%3].imshow(frame)
        ax[i//3, i%3].set_title(f"Video: {video_file}")
        ax[i//3, i%3].axis('on')

same_original_real_train_sample_video = list(train_sample_metadata.loc[train_sample_metadata.label == 'REAL'].index)
display_image_from_video_list(same_original_real_train_sample_video)

test_videos = pd.DataFrame(list(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER))), columns=['video'])
test_videos.head()

display_image_from_video(os.path.join(DATA_FOLDER, TEST_FOLDER, test_videos.iloc[2].video), 'path/to/save/directory')

fake_videos = list(train_sample_metadata.loc[train_sample_metadata.label=='FAKE'].index)

from IPython.display import HTML
from base64 import b64encode

def play_video(video_file, subset=TRAIN_SAMPLE_FOLDER):
    '''
    Display video
    param: video_file - the name of the video file to display
    param: subset - the folder where the video file is located (can be TRAIN_SAMPLE_FOLDER or TEST_Folder)
    '''
    video_url = open(os.path.join(DATA_FOLDER, subset,video_file),'rb').read()
    data_url = "data:video/mp4;base64," + b64encode(video_url).decode()
    return HTML("""<video width=500 controls><source src="%s" type="video/mp4"></video>""" % data_url)

# Check if fake_videos list is empty before accessing elements
if fake_videos:  # This checks if the list is not empty
    play_video(fake_videos[0]) # Access the first element, if it exists
    print("The 'fake_videos' list is not empty.")
else:
    print("The 'fake_videos' list is empty. Please populate it with video file names.")

play_video(fake_videos[0])

!pip install mtcnn

!pip install lz4

import os
import cv2
import numpy as np
from mtcnn import MTCNN

def extract_faces_mtcnn_nested(input_base_folder, output_base_folder):
    detector = MTCNN()

    os.makedirs(output_base_folder, exist_ok=True)

    total_faces_extracted = 0

    # Loop through each subfolder inside the input folder
    for subfolder_name in os.listdir(input_base_folder):
        subfolder_path = os.path.join(input_base_folder, subfolder_name)

        if not os.path.isdir(subfolder_path):
            continue  # Skip if not a folder

        # Create corresponding output subfolder
        output_subfolder = os.path.join(output_base_folder, subfolder_name)
        os.makedirs(output_subfolder, exist_ok=True)

        # Loop through images inside the subfolder
        for img_name in os.listdir(subfolder_path):
            if not img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):
                continue

            img_path = os.path.join(subfolder_path, img_name)
            img = cv2.imread(img_path)

            if img is None:
                print(f"Warning: Could not read image {img_name}")
                continue

            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            faces = detector.detect_faces(img_rgb)

            if not faces:
                print(f"No face detected in {img_name}")
                continue

            valid_faces = []
            for face in faces:
                x, y, w, h = face["box"]
                aspect_ratio = w / h
                if 0.5 < aspect_ratio < 1.5:
                    valid_faces.append((x, y, w, h))

            if valid_faces:
                x, y, w, h = max(valid_faces, key=lambda f: f[2] * f[3])
                face = img[y:y+h, x:x+w]
                face = cv2.resize(face, (299, 299))
                output_img_path = os.path.join(output_subfolder, img_name)
                cv2.imwrite(output_img_path, face)
                total_faces_extracted += 1
                print(f"Processed {img_name}, saved to {output_subfolder}")
            else:
                print(f"No valid face detected in {img_name}")

    print(f"\nTotal Faces Extracted: {total_faces_extracted}")

# Example usage
input_folder = "/content/drive/Shareddrives/DATASET/deepfake-detection-challenge/extracted_frames"
output_folder = "/content/drive/Shareddrives/DATASET/deepfake-detection-challenge/Cropped_faces"
extract_faces_mtcnn_nested(input_folder, output_folder)

import numpy as np
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import os

def preprocess_faces(input_folder, img_size=(299, 299)):
    """
    Recursively resizes and normalizes extracted face images from subdirectories.
    """
    processed_faces = []

    for root, dirs, files in os.walk(input_folder):
        for img_name in sorted(files):
            if not img_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                continue  # Skip non-image files

            img_path = os.path.join(root, img_name)

            try:
                # Load and resize image
                img = load_img(img_path, target_size=img_size)
                img_array = img_to_array(img) / 255.0  # Normalize
                processed_faces.append(img_array)
            except Exception as e:
                print(f"Error processing {img_path}: {e}")
                continue

    return np.array(processed_faces)

# Apply preprocessing
input_folder = "/content/drive/Shareddrives/DATASET/deepfake-detection-challenge/Cropped_faces"
preprocessed_faces = preprocess_faces(input_folder)
print(f"Preprocessed Faces Shape: {preprocessed_faces.shape}")

# IMG_SIZE = 224
# BATCH_SIZE = 32
# EPOCHS = 10

# MAX_SEQ_LENGTH = 20
# NUM_FEATURES = 2048

IMG_SIZE = 299
BATCH_SIZE = 32
EPOCHS = 20

MAX_SEQ_LENGTH = 20
NUM_FEATURES = 2048

def crop_center_square(frame):
    y, x = frame.shape[0:2]
    min_dim = min(y, x)
    start_x = (x // 2) - (min_dim // 2)
    start_y = (y // 2) - (min_dim // 2)
    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]


def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):
    cap = cv2.VideoCapture(path)
    frames = []
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frame = crop_center_square(frame)
            frame = cv2.resize(frame, resize)
            frame = frame[:, :, [2, 1, 0]]
            frames.append(frame)

            if len(frames) == max_frames:
                break
    finally:
        cap.release()
    return np.array(frames)

def build_feature_extractor():
    feature_extractor = keras.applications.Xception(
        weights="imagenet",
        include_top=False,
        pooling="avg",
        input_shape=(IMG_SIZE, IMG_SIZE, 3),
    )
    preprocess_input = keras.applications.xception.preprocess_input

    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))
    preprocessed = preprocess_input(inputs)

    outputs = feature_extractor(preprocessed)
    return keras.Model(inputs, outputs, name="feature_extractor")

feature_extractor = build_feature_extractor()

def prepare_all_videos(df, root_dir):
    num_samples = len(df)
    video_paths = list(df.index)
    labels = df["label"].values
    labels = np.array(labels=='FAKE').astype(int) # Changed np.int to int

    # `frame_masks` and `frame_features` are what we will feed to our sequence model.
    # `frame_masks` will contain a bunch of booleans denoting if a timestep is
    # masked with padding or not.
    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype="bool")
    frame_features = np.zeros(
        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32"
    )

    # For each video.
    for idx, path in enumerate(video_paths):
        # Gather all its frames and add a batch dimension.
        frames = load_video(os.path.join(root_dir, path))
        frames = frames[None, ...]

        # Initialize placeholders to store the masks and features of the current video.
        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
        temp_frame_features = np.zeros(
            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32"
        )

        # Extract features from the frames of the current video.
        for i, batch in enumerate(frames):
            video_length = batch.shape[0]
            length = min(MAX_SEQ_LENGTH, video_length)
            for j in range(length):
                temp_frame_features[i, j, :] = feature_extractor.predict(
                    batch[None, j, :]
                )
            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

        frame_features[idx,] = temp_frame_features.squeeze()
        frame_masks[idx,] = temp_frame_mask.squeeze()

    return (frame_features, frame_masks), labels

from sklearn.model_selection import train_test_split

Train_set, Test_set = train_test_split(train_sample_metadata,test_size=0.1,random_state=42,stratify=train_sample_metadata['label'])

print(Train_set.shape, Test_set.shape )

train_data, train_labels = prepare_all_videos(Train_set, "train")
test_data, test_labels = prepare_all_videos(Test_set, "test")

print(f"Frame features in train set: {train_data[0].shape}")
print(f"Frame masks in train set: {train_data[1].shape}")

import tensorflow as tf
from tensorflow import keras

# Define input layers
frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))
mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype="bool")

# Define LSTM layers with Batch Normalization
x = keras.layers.LSTM(64, return_sequences=True)(frame_features_input, mask=mask_input)
x = keras.layers.BatchNormalization()(x)
x = keras.layers.LSTM(32, return_sequences=True)(x)
x = keras.layers.BatchNormalization()(x)
x = keras.layers.LSTM(16)(x)
x = keras.layers.Dropout(0.5)(x)

# Fully connected layers
x = keras.layers.Dense(32, activation="relu")(x)
x = keras.layers.Dropout(0.5)(x)
output = keras.layers.Dense(1, activation="sigmoid")(x)

# Create and compile the model
model = keras.Model([frame_features_input, mask_input], output)
optimizer = keras.optimizers.Adam(learning_rate=1e-4)
model.compile(loss="binary_crossentropy", optimizer=optimizer, metrics=["accuracy"])
model.summary()

# Define callbacks
early_stopping = keras.callbacks.EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)
checkpoint = keras.callbacks.ModelCheckpoint("./my_model_weights.weights.h5", save_weights_only=True, save_best_only=True)

# Train the model
history = model.fit(
    [train_data[0], train_data[1]],
    train_labels,
    validation_data=([test_data[0], test_data[1]], test_labels),
    callbacks=[checkpoint, early_stopping],
    epochs=EPOCHS,
    batch_size=16
)

# import matplotlib.pyplot as plt

# accuracy = history.history['accuracy']
# val_accuracy = history.history['val_accuracy']
# loss = history.history['loss']
# val_loss = history.history['val_loss']
# epochs =range(1, len(accuracy)+1)

# plt.figure(figsize=(14,5))

# plt.subplot()

def prepare_single_video(frames):
    frames = frames[None, ...]
    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32")

    for i, batch in enumerate(frames):
        video_length = batch.shape[0]
        length = min(MAX_SEQ_LENGTH, video_length)
        for j in range(length):
            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])
        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

    return frame_features, frame_mask

def sequence_prediction(path):
    frames = load_video(os.path.join(DATA_FOLDER, TEST_FOLDER,path))
    frame_features, frame_mask = prepare_single_video(frames)
    return model.predict([frame_features, frame_mask])[0]

# This utility is for visualization.
# Referenced from:
# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub
def to_gif(images):
    converted_images = images.astype(np.uint8)
    imageio.mimsave("animation.gif", converted_images, fps=10)
    return embed.embed_file("animation.gif")


test_video = np.random.choice(test_videos["video"].values.tolist())
print(f"Test video path: {test_video}")

if(sequence_prediction(test_video)>=0.5):
    print(f'The predicted class of the video is FAKE')
else:
    print(f'The predicted class of the video is REAL')

play_video(test_video,TEST_FOLDER)

model.save("deepfake_video_model.h5")
print("Model saved!")

# from google.colab import files
# files.download("deepfake_video_model.h5")